#!/bin/bash 
#
#=============================================================================================
#
#         PBS Job script to process the AdriaClim MED-CORDEX scenarios
#         deviations as monthly statistics for each decade
#
#                 PBSPRO queue using the qsub command.    
#
#         by Dario B. Giaiotti
#         ARPA FVG  CRMA
#         dario.giaiotti@arpa.fvg.it
#         August 01, 2022
# 
#     Remarks: A line beginning with # is a comment.
#              A line beginning with the prefix #PBS, then it is a PBS directive.
#              PBS directives must come first; any directives after the first executable 
#              statement are ignored.
#
#              All the strings you find between double percentage, %%PARAMETER%% are parameters 
#              you should edit to run your job 
#
#              Good luck
#
#=============================================================================================
#
#         PBS  DIRECTIVES HERE BELOW
#
#
# -------------------------------------------
# --        Load user environment          --
#     Job inherits all the user environment variables, when -V is activated 
# 
##PBS -V
#
# -------------------------------------------
# --        Set permissions to the log files 
#     By default permissions of stdout and stderr files are -rw-------.
#     To change suc permissions use PBS -W option as for umask.
#     If you also need to modify permissions to a project group you need: #PBS -W group_list=project01
#     Note that the this does NOT affect output files written by the programs you start in the job script
#
#     Here is the setting for   -rw-rw-r-- 
#PBS -W umask=0002
# 
#
# -------------------------------------------
# --        Specify the Project name       --
#      User underscores to replace spaces
#
#PBS -P AdriaClim     
#
# 
# -------------------------------------------
# --        Define the job name            --
#      User underscores to replace spaces
#
#PBS -N  climate_statistics        
#
# -------------------------------------------
# --        Select shell to be used        --
#      Request Bourne shell as shell for job
#
#PBS -S /bin/bash
#
# -------------------------------------------
# --        Define the queue name          --
#      Run in the queue named "hp"
#         example   #PBS -q hp
#PBS -q arpa
# -------------------------------------------
# --     Define the path for the standard output, error and input of the batch job.   --- 
#                Environment variables can be used to generare the file names     
#                othr whise the default filename  has the following format:
#                             <job_name>.o<sequence_number> for standard output
#                             <job_name>.e<sequence_number> for standard error 
#                 
#                It is possible to join the stdout and the stderr in one file. This is done 
#                using the option -j as follow:
#
#                -j  join_list
#
#              join_list defines which streams of  the batch job are to be merged.                 
#
#               e     The standard error of the batch job (JOIN_STD_ERROR).
#               o     The standard output of the batch job (JOIN_STD_OUTPUT).
#               n     NO_JOIN
#
#               If 'n' is specified, then no files are joined. The qsub utility shall consider it an  error  if  any
#               join type other than 'n' is combined with join type 'n' .
#                 
#                 
#PBS -j n
#                 
#
#        To allow the stdout and stderr to be written immedialtely on the output files
#        during the run, avoiding to wait the end of the job to have all the onformation
#        available at the end, yhe activate the -k option 
#
#        The keep argument can take on the following values:
# 
#                d    direct_write option is used to output (stdout/stderr) files to the final destination, 
#                     instead of being staged, if the final destination is known to be writable from the 
#                     job execution node.
# 
#                e    "k/d" option is applicable to stderr file. The filename is
#                          job_name.e<sequence number>
# 
#                o    "k/d" option is applicable to stdout file. The filename is
#                          job_name.o<sequence number>
# 
#                n    "k/d" option is not applicable.
# 
#
#        "k"  specifies  whether  and which of the standard output and standard error 
#         streams is retained on the execution host.  Direct_write can be enabled by passing "d" option 
#         (This will override "k" functionality).  Overrides default path names for these  streams.   
#         Sets the job's Keep_Files attribute to the keep
#               
#         Format: Any combination of 'd','o','e' or 'n'. 'n' cannot be used with other options.
#         Default: n
#
##PBS -k d
#
#
# -------------------------------------------
#
#   E-mail definition just for test in the preliminary phases 
#
#PBS -m bea
#PBS -M dario.giaiotti@arpa.fvg.it     
#
# ------------------------------------------------------------
# --     Defines the resources required for the job        ---
#
#          Your job can request resources that apply to the entire job, or resources that apply to job chunks.
#          If one job process needs two CPUs and another needs 8 CPUs, your job can request two chunks, one 
#          with two CPUs and one with eight CPUs. 
#
#          All of a chunk is taken from a single host.
#
#          Your job cannot request the same resource in a job-wide request and a chunk-level request.
#          A resource is either job-wide or chunk-level, but not both. 
#
#          Job-wide resources are requested in <resource neme>=<value> pairs. 
#          You can request job-wide resources using any of the following formats:
#                 #PBS  -l <resource>=<value>,<resource>=<value>
#                 #PBS -l <resource>=<value> -l <resource>=<value>
#                 One or more #PBS -l <resource name>=<value> directives
#
#          Remember that in reserving cpus it is importnat to remind the meaning of the keywords:
#
#          nodes      = refers to the physical computational nodes and it is deprecated, but it works;
#          ppn        = refers to the processes (or parallel tasks) per node;
#          select     = refers to the physical computational nodes;
#          ncpus      = refers to the physical cpus per node;
#          mpiprocs   = refers to the cpus per node allocated for the mpi;
#          ompthreads = refers to the number of threads in OpenMP.
#
#          This is very important when mpirun is used with $PBS_NODEFILE, because the content of the file
#          $PBS_NODEFILE should match the number N in mpirun -p N when mpirun is invoked 
#
#          The old format  
#                       #PBS -l nodes=3:ppn=32 
#          still works and generated the 3x32 lines in the $PBS_NODEFILE requird as N in the mpirun -p N
#
#          The new format is described here below keeping in mind the following definitions:
#
#              select=# -- allocate # separate nodes
#              ncpus=# -- on each node allocate # cpus (cores)
#              mpiprocs=# -- on each node allocate # cpus (of the ncpus allocated) to MPI
#              ompthreads==# -- on each node run # of threads in  OpenMP (it sets OMP_NUM_THREADS=#) 
#
#          Example on how the system allocates processor cores for MPI jobs.
#
#          #PBS -l select=8:ncpus=8:mpiprocs=8
#
#          The above example allocates 64 cores all of which are for use by MPI (8 nodes with 8 cpus on each node).
#         
#
#          Example on how the system allocates processor cores for OpenMP jobs.
#
#          PBS Professional supports OpenMP applications by setting the OMP_NUM_THREADS variable in the job's 
#          environment, based on the resource request of the job. The OpenMP run-time picks up the value of 
#          OMP_NUM_THREADS and creates threads appropriately.
#
#          #PBS -l select=2:ncpus=10:ompthreads=10
#
#          The above example allocates 20 cores all of which are for use by OpenMP 20 threads 
#          (2 nodes with 10 cpus on each node executing 10 threads each).
#         
#
#          Example on how the system allocates processor cores for mixed MPI + OpenMP jobs.
#
#          #PBS -l select=2:ncpus=16:mpiprocs=8:ompthreads=2 
#
#          The above example allocates 32 cores over 2 nodes (16 on each node) to be used as  
#          8 MPI processes on each node and each MPI has 2 OpenMPI threads 
#          This would automatically set the environment variable OMP_NUM_THREADS=2 and it can easily be seen 
#          when using OpenMP with MPI that ompthreads multiplied by mpiprocs should equal ncpus if you want 
#          all MPI tasks to each have the same number of threads.
#
#
#
#          Chunk resources are requested in chunk specifications in a select statement. 
#          You can request chunk resources using any of the following:
#                 #PBS -l select=[N:][chunk specification][+[N:]chunk specification] directive
#                 If you do not specify N, the number of chunks, it is taken to be 1.
#                 For example, one chunk might have 2 CPUs and 4GB of memory:
#                 #PBS -l select=ncpus=2:mem=4gb
#                 For example, to request six of the previous chunk:
#                 #PBS -l select=6:ncpus=2:mem=4gb
#                 To request different chunks, concatenate the chunks using the plus sign (+)
#                 For example, to request two sets of chunks where one set of 6 chunks has 2 CPUs per chunk, 
#                 and one set of 3 chunks has 8 CPUs per chunk, and both sets have 4GB of memory per chunk:
#                 #PBS -l select=6:ncpus=2:mem=4gb+3:ncpus=8:mem=4GB
#
#                 No spaces are allowed between chunks.
#                 You must specify all your chunks in a single select statement.
#
#           HERE BELOW THE RESOURCES AT chunck-level
#
##PBS -l select=1:ncpus=1:mpiprocs=1:ompthreads=1
#PBS -l select=1:ncpus=1
#
#
#
#           HERE BELOW THE RESOURCES AT job-wide level 
#
#           Specify the maximum  run-time for the job, where walltime=HH:MM:SS
#           example:
#           #PBS -l walltime=2:00:00 (two hours at most)
#
#PBS -l walltime=01:00:00
#
# ------------------------------------------------------------
# --     Submits a so called Array Job, i.e. an array of identical tasks being differentiated only   --- 
#                by an index number and being treated by Grid Engine almost like a
#                series of jobs. 
#                
#                Araay elements are defined as a comma separated list 
#                     #PBS -J 1,3,12,33,55
#                or as a continuous series (at present no strides have been tested)
#                     #PBS -J n-m   (e.g. -t 12-50 taht iis from 12 to 50) 
#
#               An  optional  slot  limit  can be specified to limit the amount of jobs that can run concur-
#               rently in the job array. The default value is unlimited. The slot limit  must  be  the  last
#               thing  specified in the array_request and is delimited from the array by a percent sign (%).
#
#                     #PBS -J 0-299%5
#
#               This sets the slot limit to 5. Only 5 jobs from this array can run at the same time.
#
#               Note: You can  use  qalter  to  modify  slot  limits  on  an  array.  The  server  parameter
#               max_slot_limit can be used to set a global slot limit policy.
#
#                     Examples are: -J 1-100%10 or -t 1,10,50-100%7
#
#
#             For array jobs environmenta variablles are
#             Variable name     Used For          Description
#
#               
#             PBS_JOBID        Jobs, subjobs     Identifier for a job or a subjob. For subjob,
#                                                sequence number and subjob index in brackets
#                                                (e.g. 2877[10].grid0.mercuriofvg.it)
#             PBS_ARRAYID      subjobs           Identifier for a job array. Sequence number
#
#                Example of using different input and output file for the the standard input stream of the job.
#  
#                program -i ~/data/input.$PBS_ARRAYID -o ~/results/output.$PBS_ARRAYID -e ~/results/errors.$PBS_ARRAYID
#
#             Remember that the jobid for an arry job is always  $PBS_JOBID[$PBS_ARRAYID]  also for deleting job
#             to delete all jobs belonging to the same $PBS_JOBID use qdel $PBS_JOBID[] 
#                
#             No stride are possible                 
#
#PBS -J 0-5
#
#
# -------------------------------------------------------------------
# --     Defines whether qsub shuld wait until the job ends       ---
#
#    The block job attribute controls blocking. 
#
#    Normally, when you submit a job, the qsub command exits after returning the ID of the new job. 
#    You can use the "-W block=true" option to qsub to specify that you want qsub to "block", 
#    meaning wait for the job to complete and report the exit value of the job.
#
#    If your job is successfully submitted, qsub blocks until the job terminates or an error occurs. 
#    If job submission fails, no special processing takes place.
#    If the job runs to completion, qsub exits with the exit status of the job. 
#    For job arrays, blocking qsub waits until the entire job array is complete, then returns 
#    the exit status of the job array.
#
#     #PBS -W block=true    ----> wait until the job ends
#     #PBS -W block=fasle   ----> return immediately after returning the ID of the new job
#
#PBS -W block=false
#
#
#
#         PBS  DIRECTIVES HERE ABOVE
#
#=============================================================================================
#
#
#         START SOME DIAGNOSTIC FOR THE ENVIRONMENT
#
     echo -e "\n\n==============   ENVIRONMENT HERE BELOW ======================\n"
     echo -e "\n\n+================================================="
     echo -e "!             Here are some environment varaiables   " 
     echo -e "!                                                    " 
                 env  | grep -v "PBS_" | sed s/^/"!         "/g
     echo -e "!                                                    " 
     echo -e "+=====================================================\n\n"

cat <<EOF

+=============================================================================================  
|  
| LIST OF PBS ENVIRONMET VARIABLES YOU CAN USE IN THIS SCRIPT
|  
| General and environmental
|  
| Variable Name     Value at qsub Time
| PBS_VERSION=$PBS_VERSION
| PBS_ENVIRONMENT=$PBS_ENVIRONMENT
| PBS_O_LANG=$PBS_O_LANG
| PBS_O_TZ=$PBS_O_TZ
| PBS_O_PATH=$PBS_O_PATH
| PBS_O_SHELL=$PBS_O_SHELL
|  
| Host, queue and servers information
|  
| Variable Name     Value at qsub Time
| PBS_O_HOST=$PBS_O_HOST
| PBS_O_SERVER=$PBS_O_SERVER
| PBS_NODEFILE=$PBS_NODEFILE
| PBS_MOMPORT=$PBS_MOMPORT
| PBS_O_QUEUE=$PBS_O_QUEUE
| PBS_O_WORKDIR=$PBS_O_WORKDIR
| PBS_O_INITDIR=$PBS_O_INITDIR
| PBS_O_ROOTDIR=$PBS_O_ROOTDIR
| PBS_TMPDIR=$PBS_TMPDIR
|  
| Job identification
|  
| Variable Name     Value at qsub Time
| PBS_JOBNAME=$PBS_JOBNAME
| PBS_JOBID=$PBS_JOBID
| Array Jobs
|  
| Variable Name     Value at qsub Time
| PBS_ARRAY_INDEX=$PBS_ARRAY_INDEX
| PBS_ARRAY_ID=$PBS_ARRAY_ID
|  
| User identification
|  
| Variable Name     Value at qsub Time
| PBS_O_HOME=$PBS_O_HOME
| PBS_O_LOGNAME=$PBS_O_LOGNAME
| PBS_O_MAIL=$PBS_O_MAIL
|  
+=============================================================================================  

       EXPORTED VARIABLES IN THE JOB 

       Environment variables beginning with "PBS_O_" are created by qsub.  
       PBS automatically exports the following environment variables to  the
       job, and the job's Variable_List attribute is set to this list:

       PBS_ENVIRONMENT=$PBS_ENVIRONMENT
            Set to PBS_BATCH for a batch job.  Set to PBS_INTERACTIVE for an interactive job.  
            Created upon execution.

       PBS_JOBDIR=$PBS_JOBDIR
            Pathname of job's staging and execution directory on the primary execution host.

       PBS_JOBID=$PBS_JOBID
            Job identifier given by PBS when the job is submitted.  
            Created upon execution.

       PBS_JOBNAME=$PBS_JOBNAME
            Job name given by user.  
            Created upon execution.

       PBS_NODEFILE=$PBS_NODEFILE
            Name of file containing the list of nodes assigned to the job.  
            Created upon execution.

       PBS_O_HOME=$PBS_O_HOME
            User's home directory.  
            Value of HOME taken from user's submission environment.

       PBS_O_HOST=$PBS_O_HOST
            Name of submit host.  
            Value taken from user's submission environment.

       PBS_O_LANG=$PBS_O_LANG
            Value of LANG taken from user's submission environment.

       PBS_O_LOGNAME=$PBS_O_LOGNAME
            User's login name.  
            Value of LOGNAME taken from user's submission environment.

       PBS_O_MAIL=$PBS_O_MAIL
            Value of MAIL taken from user's submission environment.

       PBS_O_PATH=$PBS_O_PATH
            User's PATH.  
            Value of PATH taken from user's submission environment.

       PBS_O_QUEUE=$PBS_O_QUEUE
            Name of the queue to which the job was submitted.  
            Value taken from user's submission environment.

       PBS_O_SHELL=$PBS_O_SHELL
            Value taken from user's submission environment.

       PBS_O_SYSTEM=$PBS_O_SYSTEM
            Operating system, from uname -s, on submit host.  
            Value taken from user's submission environment.

       PBS_O_TZ=$PBS_O_TZ
            Value taken from user's submission environment.

       PBS_O_WORKDIR=$PBS_O_WORKDIR
            Absolute path to directory where qsub is run.  
            Value taken from user's submission environment.

       PBS_QUEUE=$PBS_QUEUE
            Name of the queue from which the job is executed.  
            Created upon execution.
  
       TMPDIR=$TMPDIR
            Pathname of job s scratch directory.


+=============================================================================================  

EOF
     echo -e "\n\n==============   ENVIRONMENT HERE ABOVE ======================\n\n\n\n"
#
#
#         END OF SOME DIAGNOSTIC FOR THE ENVIRONMENT
#
#=============================================================================================
#
#         
#         +---------------------------------------------------+
#         |                                                   |
#         |     Here below some preliminary activity that     |
#         |     may be usefuls for the execution of your      |
#         |     script. If you want you can comment these     |
#         |     lines here below.                             |
#         |                                                   |
#         +---------------------------------------------------+
#
#
#
         cat << EOF
         +---------------------------------------------------+
         |                                                   |
         |     Here below some preliminary activity that     |
         |     may be usefuls for the execution of your      |
         |     script. If you want you can comment these     |
         |     lines in the script template.                 |
         |                                                   |
         +---------------------------------------------------+
EOF
#
#         Switch to the working directory
#
          cd $PBS_O_WORKDIR
          echo -e "\n\tWorking directory is: $PBS_O_WORKDIR"
          echo -e "\tWorking queue is: $PBS_QUEUE"
          echo -e "\n\n\t====== END OF PRELIMINARY ACTIVITY ======\n\n\n"
#         
#         +---------------------------------------------------+
#         |                                                   |
#         |     Here is an help omn how to run parallel       |
#         |     and serial programs.                          |
#         |                                                   |
#         +---------------------------------------------------+
#
#         For parallel executables (e.g. my-openmpi-program) do as follow.
#         OpenMPI will automatically launch processes on all allocated nodes.
#
#           MPIRUN=$(which mpirun) 
#           ${MPIRUN} -machinefile $PBS_NODEFILE -np ${NPROCESSES} my-openmpi-program
#
#
#         For serial executables (e.g. $HOME/my-program) do as follow.
#
#           $HOME/my-program
#
#
#         *****************************************************
#         *                                                   *
#         *                                                   *
#         *           HERE BELOW YOUR SCRIPT STARTS           *
#         *                                                   *
#         *                                                   *
#         *****************************************************
#
#         Some suggestion on the exit status. It would be helpful to manage 
#         the exist status to have only two possible values. They are:
#
#            0 = job executed correctly
#            1 = impossible to do the job as expected
#
#
#
# -----------------------------------------------------------------
#
#         LOAD MODULES REQUIRED FOR THIS JOB
#
#     module purge   # Clean all modules that may be loaded
      # Load modules you need
      module load intel-parallel-studio/composer.2020.1/gcc/8.2.0-nsspynk
      # List modules (for diagnostic purposes only)
      module list   
#     module show    # Show wath modules manage (for diagnostic purposes only)
#
#
#
#         DEFINE DEBUG OPTIONS AND ERROR TRAPPING
#
#
      # The trap command allows you to execute a command when a signal is received by your script.
      #
      # The trap command catches the listed SIGNALS, which may be signal names with 
      # or without the SIG prefix, or signal numbers. 
      # If a signal is 0 or EXIT, the COMMANDS are executed when the shell exits. 
      # If the signal is specified as ERR; in that case COMMANDS are executed each 
      # time a simple command exits with a non-zero status. 
      # Note that these commands will not be executed when the non-zero exit status comes 
      # from part of an if statement, or a while or until loop. Neither will they be executed 
      # if a logical AND (&&) or OR (||) result in a non-zero exit code, or when a command 
      # return status is inverted using the ! operator.

      #
      # This is the defaul trap to stop when any command exits without zero.
      # Please read the above lines for more details on if loops and so on limits
      #
      ERROR() { echo -e "\n\tERROR! Job stopped with any error\n\n"; exit 1;}
      trap 'echo -e "\n\n\tKilled by signal"; ERROR;' ERR 

      #
      # Other possible trapps you may want are 
      #
      #  trap 'echo -e "\n\n\tKilled by signal"; ERROR;' 1 2 3 4 5 6 7 8 10 12 13 15 # for a set of signals only.
      #  trap ERROR 0                     # call the function ERROR on exit od the shell. Usefull to clean up on exit.
      #
      # The whole list of sigspec can be accessed by kill -l or man kill or trap -l
      #

      set -e                   # stop the shell on first error
      set -u                   # fail when using an undefined variable
      # set -x                   # echo script lines as they are executed
#
# -----------------------------------------------------------------
#
#
#                     CONSTANTS AND PARAMENTERS
#
#     Remember to define the default exit status parameter EXIT_STATUS
#
      EXIT_STATUS=0
      STAT_CODE_SRC="/lustre/arpa/scratch/giaiottid/MED-CORDEX_stats/AdriaClim/src/codes/postproc/simple_statistics.f90"
      STAT_CODE_EXE="./simple_statistics.exe"
      ROOT_DATA_DIR="/lustre/arpa/AdriaClim/public_html/Med_CORDEX_analysis/SCENARIO/csv_files_ln/"
      PLOW="0.05"  # Lower percentile for statistics
      PHIG="0.95"  # Upper percentile for statistics
      declare -a VARS=(mean_S mean_T mean_Z median_S median_T median_Z) # Files to me considered (head only)
      #declare -a MESI=(NULL Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec) # English version
      declare -a MESI=(NULL Gennaio Febbraio Marzo Aprile Maggio Giugno Luglio Agosto Settembre Ottobre Novembre Dicembre) # Italian version
      declare -a STAR=(NTOT NVAL NMIS AVE MIN MAX STD MEDIAN PLOW PHIGH SUM) # The statistica fields computed 
      
#
# -----------------------------------------------------------------
#
#
#                     FUNCTIONS HERE BELOW    
#
# !!! Be careful in using finction if trap are activated because you may  !!!
# !!! miss something. Otherwise you have to explicitly repeat the trap in !!! 
# !!! the functions to ensure portability                                 !!!
#
# -----------------------------------------------------------------
#
#
#                     MAIN SCRIPT HERE BELOW    
#
    # 
    #            Some checks on needed stuff before to start 
    #
    echo -e "\n\n\tSOME CHECKS ON THE NEEDED STUFF BEFORE TO START\n"
    # 
    # Statistical source code availabilitya check
    #
    if [[ ! -e ${STAT_CODE_SRC} ]];then
       echo -e "\n\n\tERROR! Statistical source code NOT available. I should be:"
       echo -e "\t${STAT_CODE_SRC}\n\tEXIT\n\n"
       EXIT_STATUS=1; exit $EXIT_STATUS
    else
       echo -e "\tStatistical source code is available. It is:\n\t${STAT_CODE_SRC}\n"
    fi
    #
    # Create temporary directory and go there to work
    #
    TMP_DIR="$(mktemp -u AdriaClim_statistics.XXXXX)"
    mkdir $TMP_DIR
    cd $TMP_DIR
    echo -e "\n\tCurrently I am in $(pwd)\n"
    #
    # Code compilation and verification                  
    #
    ifort -w -o ${STAT_CODE_EXE} ${STAT_CODE_SRC}
    if [[ ! -e ${STAT_CODE_EXE} ]];then
       echo -e "\n\n\tERROR! Statistical e executable NOT available. I should be:"
       echo -e "\t${STAT_CODE_EXE}\n\tEXIT\n\n"
       EXIT_STATUS=2; exit $EXIT_STATUS
    else
       echo -e "\tStatistical executable is available. It is:\n\t${STAT_CODE_EXE}\n"
    fi 
    #
    # Define the variable to be used for this Array elementa of this job
    #
    NV=$(($PBS_ARRAY_INDEX))
    VARF=${VARS[$NV]}
    echo -e "\n\tThis Array element ($PBS_ARRAY_INDEX) processes variable: $VARF"
    FINI="${ROOT_DATA_DIR}/${VARF}.csv"
    FOUT="${VARF}_full_monthly_stat.csv"
    if [[ -e ${FINI} ]];then
       echo -e "\n\tInput file found. It is: $FINI"
    else
       echo -e "\n\tInput file: $FINI   NOT Found\n\tEXIT\n\n"
       EXIT_STATUS=3; exit $EXIT_STATUS
    fi
    #
    # Define the header common to all the output files
    #
    COM_HEAD="$(mktemp -u common_head.XXXXX)"
    grep "^#" $FINI | head -n 4 > $COM_HEAD
    # Get missing value
    MISS="$(grep "^#" $FINI | head -n 4 | tail -n 1 | cut -d "=" -f 2)"; MISS="$(echo $MISS)"
    echo -e "\n\tStart to work on field:"
    cat $COM_HEAD | sed s/^/\\t/g
    echo -e "\n\tMissing value is: $MISS"
    # Get information of each column and preserve those to be in output file
    LINE="$(grep "^#" $FINI | head -n 5 | tail -n 1)"
    FIRST_LINE="$(echo $LINE | cut -d ";" -f 1-3,5,6);Statistics"
    # Add columns fo months
    for I in $(seq 1 12);do
        FIRST_LINE="${FIRST_LINE};${MESI[$I]}"
    done
    FIRST_LINE="${FIRST_LINE};NTOT;NVAL;NMIS"
    VAR_NAME="$(echo $LINE | cut -d ";" -f 8)"
    # Add information on output file common header
    ESTIMATOR="${VARF%%_*}"; 
    echo -e "# Med-CORDEX ensemble statistics of ${ESTIMATOR} monthly deviations with respect to 2010รท2020 decade" >> $COM_HEAD
    echo -e "# Field extended description and [units]: ${VAR_NAME}" >> $COM_HEAD
    echo -e "# Lower percentile for statistics (PLOW): $PLOW" >> $COM_HEAD
    echo -e "# Upper percentile for statistics (PHIGH): $PHIG" >> $COM_HEAD
    echo -e "#" >> $COM_HEAD
    echo -e "$FIRST_LINE" >> $COM_HEAD
    #
    # Get information on the fields to loop on (Grid point, RCP, Decade, Month)
    #
    #  (Grid point)
    GP="$(grep -v "#" $FINI | cut -d ";" -f 1 | sort -u | tr "\n" " ")"
    echo -e "\n\tNumber of Grid points: $(echo $GP | wc -w)\n\tThey are: ${GP}"
    #  (RCP)
    RC="$(grep -v "#" $FINI | cut -d ";" -f 5 | sort -u |  tr "\n" " ")"
    echo -e "\n\tNumber of RCPs: $(echo $RC | wc -w)\n\tThey are: ${RC}"
    #  (Decade)
    DE="$(grep -v "#" $FINI | cut -d ";" -f 6 | sort -u |  tr "\n" " ")"
    echo -e "\n\tNumber of Decades: $(echo $DE | wc -w)\n\tThey are: ${DE}"
    #  (Month)
    MO="$(grep -v "#" $FINI | cut -d ";" -f 7 | sort -u |  tr "\n" " ")"
    echo -e "\n\tNumber of Months: $(echo $MO | wc -w)\n\tThey are: ${MO}"
    #
    # Initialize the output file 
    #
    cat $COM_HEAD > $FOUT
    # 
    # LOOP OVER THE GRID POINTS
    #
    for G in $GP;do
        T0=$(date -u +%s)
        echo -e "\n\tProcessing Grid point: $G - start at: $(date -u)"
        FTMP="$(mktemp -u point_${G}_tmp.XXXXX)"
        grep "^${G};" $FINI > $FTMP
        GCO="$(grep "^${G};" $FTMP | head -n 1 | cut -d ";" -f 1-3)"
        echo -e "\tNumber of selected records are: $(cat $FTMP | wc -l) - for ${GCO}"
        # 
        # LOOP OVER THE RCPs 
        #
        for R in $RC;do
            echo -e "\tProcessing RCP: $R"
            # 
            # LOOP OVER THE DECADES
            #
            for D in $DE;do
                echo -e "\t\tProcessing Decade: $D"
                #          STAR=(NTOT NVAL NMIS AVE MIN MAX STD MEDIAN PLOW PHIGH SUM) # The statisticail fields computed 
                declare -a STAV=("" "" "" "" "" "" "" "" "" "" "")          # Initialization of monthly statistics 
                # 
                # LOOP OVER THE MONTHS
                #
                for M in $MO;do
                    echo -e "\t\t\tProcessing Month: $M --> ${MESI[10#${M}]}"
                    STRINGA=$(grep ";${R};" $FTMP | grep ";${D};" | grep ";${M};" | cut -d ";" -f 8 | ${STAT_CODE_EXE} -o ${PLOW} ${PHIG} -m "${MISS}")
                    NSV=0; while [[ $NSV -lt ${#STAR[@]} ]];do STAV[$NSV]="${STAV[$NSV]};$(echo $STRINGA | cut -d " " -f $(($NSV +1 )))"; NSV=$(($NSV+1));done
                done 
                # 
                # WRITE RESULTS ON OUTPUT FILE
                #
                COUNT="$(echo ${STAV[0]} | cut -d ";" -f 2)"           # Get first value for NTOT
                COUNT="${COUNT};$(echo ${STAV[1]} | cut -d ";" -f 2)"  # Get first value for NVAL
                COUNT="${COUNT};$(echo ${STAV[2]} | cut -d ";" -f 2)"  # Get first value for NMIS
                NSV=3; 
                while [[ $NSV -lt $((${#STAR[@]}-1)) ]];do 
                      echo -e "${GCO};${R};${D};${STAR[$NSV]}${STAV[$NSV]};${COUNT}" >> ${FOUT} 
                      NSV=$(($NSV+1))
                done
            done 
        done 
        rm  $FTMP
        T1=$(date -u +%s)
        echo -e "\tProcessing Grid point: $G - completed at: $(date -u) -- Elapsed time $(($T1 - $T0)) seconds"
    done
    #
    # Split files according to the statistics
    #
    NSV=3; 
    while [[ $NSV -lt $((${#STAR[@]}-1)) ]];do 
          FOUT_STAT="${STAR[$NSV]}_${FOUT}"
          echo -e "Splitting file for statistics: ${STAR[$NSV]} - out file: $FOUT_STAT" 
          grep  "^#" $FOUT > ${FOUT_STAT} 
          grep  ";${STAR[$NSV]};" $FOUT >> ${FOUT_STAT} 
          NSV=$(($NSV+1))
    done
#
#   Some cleaning before to go
#
    echo -e "\n\n\tSome cleaning before to go\n"
    mv -v $FOUT ../
    mv -v *_${FOUT} ../
    cd ..
    rm -Rv $TMP_DIR
#
#
#
#
#
#                     MAIN SCRIPT HERE ABOVE    
#
# -----------------------------------------------------------------
#
#
#                     ENDING SECTION OF THE SCRIPT HERE BELOW    
#
#      Remember to set the exit status if the trap does not work 
#      exit $EXIT_STATUS
#
# -----------------------------------------------------------------
#    SOME DIAGNOSTIC INFORMATION HERE BELOW
#
#     Write the full a summary of the resources used for this job
#
      echo -e "\n\n --------------------------------------------------\n"
      echo -e "  HERE BELOW YOU FIND A SUMMARY OF THE RESOURCES HAVE BEEN\n"
      echo -e "                   USED TO RUN THIS JOB \n\n             "
      echo -e "  To get more details use the following command:  \n\n\t\tqstat -xf $PBS_JOBID"
      echo -e "\n ----------------------------------------------------\n"

      exit $EXIT_STATUS
#
################################################################################
#
# END of the job script here 
#
################################################################################
#
